{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492a0464-17f2-46d9-a9ce-77004b74a75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be88acce1ff4723bb3adc8348a0ac69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"qwen\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"qwen\",\n",
    "    torch_dtype=torch.bfloat16,  # 使用bfloat16而不是更低精度\n",
    "    attn_implementation=\"flash_attention_2\",  # 保留此优化\n",
    "    device_map=\"auto\",\n",
    "    use_cache=True  # 启用KV缓存提高速度\n",
    ")\n",
    "\n",
    "# 优化处理器\n",
    "processor = AutoProcessor.from_pretrained(\"qwen\")\n",
    "processor.tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"qwen\",\n",
    "    use_fast=True  \n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1a054c-1079-4445-bf8c-0f5a29d9c1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.185411895625293\n",
      "['```json\\n{\\n  \"IsNewsTrue\": 1,\\n  \"reasons\": [\\n    \"新闻内容详细描述了一个具体的社区碳中和示范项目的启动和实施情况，包括项目名称、地点、时间、主要措施以及预期效果。\",\\n    \"新闻引用了多个官方渠道的信息来源，如国家发改委、国网滨海分公司、社区管委会等，这些信息来源具有较高的可信度。\",\\n    \"新闻报道了项目的具体实施细节，如光伏发电设备的安装、绿色设施的部署、居民参与环保活动的情况等，这些细节增强了新闻的真实性。\",\\n    \"新闻最后引用了专家的评论，进一步支持了新闻内容的真实性和权威性。\",\\n    \"新闻没有明显的夸张或虚假信息，整体上符合新闻报道的基本要求。\"\\n  ]\\n}\\n```']\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"\"\"你是一个判别模型是否是谣言的助手，请你以json格式输出你的结果，格式：\n",
    "            IsNewsTrue：0      (0即为谣言，1即为事实)\n",
    "            reasons：       (传入一个list，里面是你的分析原因，分点)         \n",
    "            \"\"\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # {\n",
    "            #     \"type\": \"image\",\n",
    "            #     # \"image\": \"blank.jpg\",\n",
    "            # },\n",
    "            {\"type\": \"text\", \"text\": \"\"\"\n",
    "                        \n",
    "            \n",
    "            \"\"\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "\n",
    "print(end-start)\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd0b9de-6ec3-468a-b2bd-3b2af969d761",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'bfloat32'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. 使用模型并行和优化选项\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Qwen2_5_VLForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat32\u001b[49m,  \u001b[38;5;66;03m# 使用bfloat16以平衡性能和精度\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 使用flash attention提高效率\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 自动处理模型的分布\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 启用KV缓存\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 降低CPU内存使用\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2. 优化处理器\u001b[39;00m\n\u001b[1;32m     17\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/__init__.py:2003\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2003\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'bfloat32'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. 批处理多个样本以提高利用率\n",
    "def process_batch(messages_batch, batch_size=8):\n",
    "    \"\"\"处理一批次的请求以提高GPU利用率\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # 准备批次输入\n",
    "    all_texts = []\n",
    "    all_image_inputs = []\n",
    "    all_video_inputs = []\n",
    "    \n",
    "    for messages in messages_batch:\n",
    "        \n",
    "        \n",
    "        all_texts.append(text)\n",
    "        all_image_inputs.extend(image_inputs if image_inputs else [None])\n",
    "        all_video_inputs.extend(video_inputs if video_inputs else [None])\n",
    "    \n",
    "    # 过滤掉None值\n",
    "    all_image_inputs = [img for img in all_image_inputs if img is not None]\n",
    "    all_video_inputs = [vid for vid in all_video_inputs if vid is not None]\n",
    "    \n",
    "    \n",
    "    # 启用CUDA图优化（用于固定输入大小的情况）\n",
    "    # torch.cuda.synchronize()  # 确保之前的操作完成\n",
    "    # g = torch.cuda.CUDAGraph()\n",
    "    # with torch.cuda.graph(g):\n",
    "    #     generated_ids = model.generate(**inputs, max_new_tokens=256, do_sample=True)\n",
    "    \n",
    "    # 如果输入大小变化，使用常规生成\n",
    "    # 4. 优化生成参数\n",
    "    \n",
    "    \n",
    "    return output_text, processing_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb90a69-6b49-4cc4-b049-45683684124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "批处理时间: 3.08秒\n",
      "平均每个请求时间: 0.77秒\n",
      "结果 1:\n",
      "```json\n",
      "{\n",
      "  \"IsNewsTrue\": 0,\n",
      "  \"reasons\": [\n",
      "    \"该信息发布日期与事件发生时间不符。\",\n",
      "    \"没有提供足够的证据支持这一描述的真实性。\",\n",
      "    \"缺乏权威来源或官方声明来验证这些细节\"\n",
      "  ]\n",
      "}\n",
      "```...\n",
      "\n",
      "结果 2:\n",
      "```json\n",
      "{\n",
      "  \"IsNewsTrue\": 0,\n",
      "  \"reasons\": [\n",
      "    \"该信息发布日期与事件发生时间不符。\",\n",
      "    \"没有提供足够的证据支持这一描述的真实性。\",\n",
      "    \"缺乏权威来源或官方声明来验证这些细节\"\n",
      "  ]\n",
      "}\n",
      "```...\n",
      "\n",
      "结果 3:\n",
      "```json\n",
      "{\n",
      "  \"IsNewsTrue\": 0,\n",
      "  \"reasons\": [\n",
      "    \"该新闻发布日期与事件发生时间不符。\",\n",
      "    \"没有提供足够的证据支持报道的真实性。\",\n",
      "    \"缺乏权威来源或官方声明来验证信息\",\n",
      "    \"可能存在误导性描述\"\n",
      "  ]\n",
      "}\n",
      "```...\n",
      "\n",
      "结果 4:\n",
      "```json\n",
      "{\n",
      "  \"IsNewsTrue\": 0,\n",
      "  \"reasons\": [\n",
      "    \"该信息发布日期与事件发生时间不符。\",\n",
      "    \"没有提供足够的证据支持这一描述的真实性。\",\n",
      "    \"缺乏权威来源或官方声明来验证这些细节的真实性\"\n",
      "  ]\n",
      "}\n",
      "```...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. 示例调用 - 通过批处理多个消息来提高GPU利用率\n",
    "news_text = \"\"\"\n",
    "\n",
    "这是一个在2012/8/22微博发布的新闻。标题是无标题，正文是@思想聚焦:【风度和教养】领导和姚明一家合影，领导们当仁不让的站到了姚明一家的前面，姚明的母亲几乎被挡住，第二批上来的是姚明火箭队的美国人，给姚明老婆和母亲全部让位。。。。。（via@广州刘云云）——2011年7月21日@薛蛮子@袁飏先生@王普会@吾评时政@时评中国@袁国宝@于建嵘，是属于文体娱乐类别的新闻。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 7. 创建多个不同的消息批次以增加GPU利用率\n",
    "def create_message_batch(system_prompt, news_content, batch_size=4):\n",
    "    \"\"\"创建多个消息批次以增加GPU利用率\"\"\"\n",
    "    message_batch = []\n",
    "    \n",
    "    # 创建不同的消息变体以填充批次\n",
    "    for i in range(batch_size):\n",
    "        # 在每个消息中添加轻微变化以促使模型工作\n",
    "        variant = f\"分析 #{i+1}: \" + news_content\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": system_prompt},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": variant},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        message_batch.append(messages)\n",
    "    \n",
    "    return message_batch\n",
    "\n",
    "# 系统提示\n",
    "system_prompt = \"\"\"你是一个判别模型是否是谣言的助手，请你以json格式输出你的结果，格式：\n",
    "IsNewsTrue：0      (0即为谣言，1即为事实)\n",
    "reasons：       (传入一个list，里面是你的分析原因，分点)         \n",
    "\"\"\"\n",
    "\n",
    "# 8. 创建和处理批次\n",
    "batch_size = 4  # 调整批次大小以最大化GPU利用率\n",
    "message_batch = create_message_batch(system_prompt, news_text, batch_size)\n",
    "output_texts, processing_time = process_batch(message_batch, batch_size)\n",
    "\n",
    "print(f\"批处理时间: {processing_time:.2f}秒\")\n",
    "print(f\"平均每个请求时间: {processing_time/batch_size:.2f}秒\")\n",
    "for i, output in enumerate(output_texts):\n",
    "    print(f\"结果 {i+1}:\\n{output[:]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ea08bf4-6c1e-4548-878b-e4f5c9dde519",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'bint8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1. 使用模型并行和优化选项\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Qwen2_5_VLForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbint8\u001b[49m,  \u001b[38;5;66;03m# 使用bfloat16以平衡性能和精度\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 使用flash attention提高效率\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 自动处理模型的分布\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 启用KV缓存\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 降低CPU内存使用\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2. 优化处理器\u001b[39;00m\n\u001b[1;32m     17\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/__init__.py:2003\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 2003\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'bint8'"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# 1. 使用模型并行和优化选项\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"qwen\",\n",
    "    torch_dtype=torch.bfloat16,  # 使用bfloat16以平衡性能和精度\n",
    "    attn_implementation=\"flash_attention_2\",  # 使用flash attention提高效率\n",
    "    device_map=\"auto\",  # 自动处理模型的分布\n",
    "    use_cache=True,  # 启用KV缓存\n",
    "    low_cpu_mem_usage=True,  # 降低CPU内存使用\n",
    ")\n",
    "\n",
    "# 2. 优化处理器\n",
    "processor = AutoProcessor.from_pretrained(\"qwen\")\n",
    "processor.tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"qwen\",\n",
    "    use_fast=True,  # 使用快速tokenizer\n",
    "    model_max_length=2048,  # 指定最大长度以优化内存使用\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"num_beams\": 1,  # 减少beam search以提高速度\n",
    "    \"do_sample\": True,  # 启用采样以加速生成\n",
    "    \"top_p\": 0.92,  # 使用nucleus sampling\n",
    "    \"temperature\": 0.8,  # 适当降低温度以提高速度\n",
    "    \"repetition_penalty\": 1.1,  # 轻微的重复惩罚\n",
    "    \"pad_token_id\": processor.tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"\"\"你是一个判别模型是否是谣言的助手，请你以json格式输出你的结果，格式：\n",
    "            IsNewsTrue：0      (0即为谣言，1即为事实)\n",
    "            reasons：       (传入一个list，里面是你的分析原因，分点)         \n",
    "            \"\"\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"blank.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"\"\"\n",
    "                        \n",
    "            ‌武汉樱花季盛大启幕：全城沉浸式赏樱体验升级‌\n",
    "\n",
    "‌东湖樱花园今日开园 早樱盛放迎客‌\n",
    "3月6日，武汉东湖樱花园正式开园，园内早樱已形成粉红花海，吸引大批游客驻足‌13。目前，中樱品种仍处于花苞期，预计3月中旬迎来盛放‌13。为提升体验，园区新增粉色主题装饰、夜间灯光秀及“樱花雨”互动场景，并首次引入牡丹盆景展与郁金香花田，打造“梅樱同赏”特色活动‌38。\n",
    "\n",
    "‌全市赏樱地图扩容 特色景点人气飙升‌\n",
    "武汉龟山公园凭借早樱品种福建山樱抢占“开春第一景”，玫红色花朵与山坡栈道相映成趣，3月2日以来日均游客量破万‌4。武汉大学樱花大道早樱初绽，中樱染井吉野预计3月中下旬盛放，校方已启动预约限流措施‌17。汉口江滩樱园早樱开放七成，未来一个月将接力呈现多品种花海‌13。此外，光谷珞喻东路大黄村公交站完成改造，透明花瓣顶棚与樱花树形成“最美车站”新地标‌3。\n",
    "\n",
    "‌本土樱花新品种亮相 科技赋能花期延长‌\n",
    "武汉市园林科研院自主培育的“楚樱”系列新品种成为亮点，其中“楚天红日”已进入盛花期，其余7个品种含苞待放‌5。该系列耐寒性强、花期提前且花量更大，已在龟山公园、东湖景区推广种植，助力武汉樱花观赏期从2月底延续至5月‌45。\n",
    "\n",
    "‌文旅融合激活“赏樱经济”‌\n",
    "武汉商圈与景区联动推出沉浸式樱花主题体验：武商MALL打造千米樱花大道，江宸天街设置8米高樱花树并定时飘落“花瓣雨”‌3。民众乐园举办樱花手账节，武汉万象城推出灯光秀与艺术展，建设大道变身“城市赏樱走廊”‌3。全国范围内，湖北代表团借两会契机向全球发出“樱花之约”，江西、昆明等地同步启动樱花节，形成春季旅游联动效应‌67。\n",
    "\n",
    "‌交通与安全措施全面升级‌\n",
    "针对客流高峰，武汉市文旅部门联合消防、公安开展“行消联动”夜巡行动，重点排查景区、商圈及老旧社区安全隐患，确保游览秩序‌2。地铁、公交增开“樱花专列”，光谷广场等枢纽增设临时导览牌，东湖景区启用分时段预约系统‌38。\n",
    "\n",
    "‌温馨提示‌\n",
    "游客需关注花期变化：早樱因近期低温可能提前凋谢，建议优先选择中晚樱观赏点；热门区域人流密集，建议错峰出行并遵守园区导引‌<image>\n",
    "\n",
    "            \"\"\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "start_time=time.perf_counter()\n",
    "\n",
    "# 5. 使用带有优化参数的generate\n",
    "with torch.cuda.amp.autocast():  # 使用自动混合精度\n",
    "    generated_ids = model.generate(**inputs, **generation_config)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# 处理输出\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "print(processing_time)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68356c82-b17f-4706-b6ff-723b746de895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
