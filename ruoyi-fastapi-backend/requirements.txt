# 基础依赖
aiofiles==23.2.1
anyio==4.4.0
APScheduler==3.11.0
asyncmy==0.2.10
bcrypt==4.3.0
beautifulsoup4==4.12.3
DateTime==5.5
fastapi==0.115.8
httptools==0.6.4
loguru==0.7.3
openpyxl==3.1.5
pandas==2.2.3
passlib==1.7.4
Pillow==11.1.0
psutil==7.0.0
pydantic==2.10.6
pydantic-validation-decorator==0.1.4
PyJWT==2.10.1
PyMySQL==1.1.1
python-multipart==0.0.20
redis==5.2.1
requests==2.32.3
SQLAlchemy==2.0.38
uvicorn==0.34.0
python-dotenv==1.1.0
user-agents==2.2.0
pydantic-settings==2.2.1
selenium==4.16.0
ninja >= 1.10.2
flash_attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.3cxx11abiFALSE-cp312-cp312-linux_x86_64.whl#sha256=e3840770d042778ae58d78805052455dd7f203b28bd66ff622b5ece8f9cb0abe



# AI 和深度学习依赖
transformers==4.49.0
sentence-transformers==4.0.2
accelerate==1.3.0
qwen-vl-utils==0.0.10
numpy==1.26.4
scikit-learn==1.6.1

# 注意：torch 和 torchvision 将通过 Dockerfile 单独安装 CUDA 版本
